{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e5fb0ac-1667-4ace-b1f0-b22b01a2b0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: I lost my phone.\n",
      "Similarity with 'phone': 0.4556\n",
      "Similarity with 'handy': 0.6671\n",
      "'Handy' could mean 'handy' — no issue detected.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel, pipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Load multilingual BERT tokenizer and model\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"model/tuned-bert-tokenizer\")\n",
    "model = BertModel.from_pretrained(\"model/tuned-bert\")\n",
    "\n",
    "# Load German-to-English translation pipeline\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-de-en\") \n",
    "\n",
    "def get_token_embedding(sentence, target_word):\n",
    "    \"\"\"\n",
    "    Get the embedding for a specific word in a sentence.\n",
    "    Handles subword tokenization by averaging embeddings of subword tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    token_ids = tokenizer.encode(sentence, return_tensors=\"pt\")  # Includes [CLS] and [SEP]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(token_ids)\n",
    "    last_hidden_state = outputs.last_hidden_state.squeeze(0)  # Shape: [seq_len, hidden_size]\n",
    "\n",
    "    word_pieces = tokenizer.tokenize(target_word)\n",
    "    indices = [i for i, tok in enumerate(tokens) if tok in word_pieces]\n",
    "\n",
    "    if not indices:\n",
    "        raise ValueError(f\"Word '{target_word}' not found in tokenized sentence: {tokens}\")\n",
    "\n",
    "    embedding = torch.stack([last_hidden_state[i + 1] for i in indices], dim=0).mean(dim=0)\n",
    "    return embedding.detach().numpy()\n",
    "\n",
    "def compare_embeddings(embedding1, embedding2):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between two embeddings.\n",
    "    \"\"\"\n",
    "    return cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "\n",
    "german_sentence = \"Ich habe mein Handy verloren.\"\n",
    "translated_sentence = translator(german_sentence)[0]['translation_text']\n",
    "print(\"Translation:\", translated_sentence)\n",
    "\n",
    "try:\n",
    "    embedding_de = get_token_embedding(german_sentence, \"Handy\")\n",
    "    embedding_en_mobile = get_token_embedding(translated_sentence, \"phone\")\n",
    "    embedding_en_handy = get_token_embedding(\"I lost my handy.\", \"handy\")\n",
    "\n",
    "    \n",
    "    sim_to_mobile = compare_embeddings(embedding_de, embedding_en_mobile)\n",
    "    sim_to_handy = compare_embeddings(embedding_de, embedding_en_handy)\n",
    "\n",
    "    print(f\"Similarity with 'phone': {sim_to_mobile:.4f}\")\n",
    "    print(f\"Similarity with 'handy': {sim_to_handy:.4f}\")\n",
    "\n",
    "    if sim_to_mobile > sim_to_handy:\n",
    "        print(\"'Handy' likely means 'phone' — false loanword detected.\")\n",
    "    else:\n",
    "        print(\"'Handy' could mean 'handy' — no issue detected.\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "54277923",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /usr/local/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating context sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:02<00:02,  2.12it/s]\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jn1j78k0ex7sr2vk46c8jfse` service tier `on_demand` on requests per day (RPD): Limit 1000, Used 1000, Requested 1. Please try again in 1m26.196s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'requests', 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 64\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m tqdm(df\u001b[38;5;241m.\u001b[39miterrows(), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(df)):\n\u001b[1;32m     63\u001b[0m     loan_word \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal_word\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 64\u001b[0m     context \u001b[38;5;241m=\u001b[39m generate_gpt_context(loan_word)\n\u001b[1;32m     65\u001b[0m     context_sentences\u001b[38;5;241m.\u001b[39mappend(context)\n\u001b[1;32m     67\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_context\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m context_sentences\n",
      "Cell \u001b[0;32mIn[75], line 37\u001b[0m, in \u001b[0;36mgenerate_gpt_context\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_gpt_context\u001b[39m(word):\n\u001b[1;32m     36\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrite a natural German very small sentence using the word \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in context.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 37\u001b[0m     answer \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39minvoke(prompt)\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# print(f\"Generated Sentence: {answer.content}\")\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m answer\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:307\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    303\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    304\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    306\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 307\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[1;32m    308\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    309\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    310\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    311\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    312\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    313\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    314\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    315\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    316\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    317\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:843\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    836\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    837\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    841\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    842\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 843\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:683\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    681\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    682\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 683\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[1;32m    684\u001b[0m                 m,\n\u001b[1;32m    685\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    686\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    687\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    688\u001b[0m             )\n\u001b[1;32m    689\u001b[0m         )\n\u001b[1;32m    690\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    691\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:908\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 908\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[1;32m    909\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    910\u001b[0m         )\n\u001b[1;32m    911\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    912\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/langchain_groq/chat_models.py:480\u001b[0m, in \u001b[0;36mChatGroq._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    476\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    479\u001b[0m }\n\u001b[0;32m--> 480\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(messages\u001b[38;5;241m=\u001b[39mmessage_dicts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/groq/resources/chat/completions.py:322\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, n, parallel_tool_calls, presence_penalty, reasoning_format, response_format, seed, service_tier, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    198\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    199\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    200\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    323\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/openai/v1/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    324\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m    325\u001b[0m             {\n\u001b[1;32m    326\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    327\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    328\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    329\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m    330\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m    333\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m    334\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    335\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m    336\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m    337\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    338\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_format,\n\u001b[1;32m    339\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m    340\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m    341\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m    342\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m    343\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    344\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    345\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    346\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    347\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m    348\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    349\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m    350\u001b[0m             },\n\u001b[1;32m    351\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    352\u001b[0m         ),\n\u001b[1;32m    353\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    354\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    355\u001b[0m         ),\n\u001b[1;32m    356\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m    357\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[1;32m    359\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/groq/_base_client.py:1266\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1254\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1261\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1262\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1263\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1264\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1265\u001b[0m     )\n\u001b[0;32m-> 1266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/groq/_base_client.py:958\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    956\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    959\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    960\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    961\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    962\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    963\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m    964\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/groq/_base_client.py:1061\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1060\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1061\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1064\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1065\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1070\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jn1j78k0ex7sr2vk46c8jfse` service tier `on_demand` on requests per day (RPD): Limit 1000, Used 1000, Requested 1. Please try again in 1m26.196s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'requests', 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from sacrebleu import sentence_bleu\n",
    "from nltk.translate.meteor_score import meteor_score  # For METEOR score\n",
    "from transformers import pipeline\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# model = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0)\n",
    "\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "#     task=\"text-generation\",\n",
    "#     max_new_tokens=512,\n",
    "#     do_sample=False,\n",
    "#     repetition_penalty=1.03,\n",
    "# )\n",
    "\n",
    "# model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "\n",
    "model = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0)\n",
    "\n",
    "df = pd.read_csv('data/production_train_test/English-German/balanced/English-German-train_production_balanced.csv')\n",
    "df = df.head(10)  \n",
    "\n",
    "def generate_gpt_context(word):\n",
    "    prompt = f\"Write a natural German very small sentence using the word '{word}' in context.\"\n",
    "    answer = model.invoke(prompt)\n",
    "    # print(f\"Generated Sentence: {answer.content}\")\n",
    "    return answer.content\n",
    "\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-de-en\")\n",
    "\n",
    "def calculate_bleu(reference, hypothesis):\n",
    "    bleu_score = sentence_bleu(hypothesis, [reference]).score / 100  # Normalize to [0, 1]\n",
    "    return bleu_score\n",
    "\n",
    "def calculate_meteor(reference, hypothesis):\n",
    "    # Tokenize sentences into lists of words\n",
    "    reference_tokens = reference.split()\n",
    "    hypothesis_tokens = hypothesis.split()\n",
    "    meteor = meteor_score([reference_tokens], hypothesis_tokens)\n",
    "    return meteor\n",
    "\n",
    "def generate_reference_sentence(sentence):\n",
    "    prompt = f\"Translate this German sentence: '{sentence}' to English. Write only the translated sentence, nothing else.\"\n",
    "    answer = model.invoke(prompt)\n",
    "    # print(f\"Reference Sentence: {answer.content}\")\n",
    "    return answer.content\n",
    "\n",
    "context_sentences = []\n",
    "print(\"Generating context sentences...\")\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    loan_word = row['original_word']\n",
    "    context = generate_gpt_context(loan_word)\n",
    "    context_sentences.append(context)\n",
    "\n",
    "df['generated_context'] = context_sentences\n",
    "\n",
    "bleu_scores = []\n",
    "meteor_scores = []\n",
    "translated_sentence = []\n",
    "ref = []\n",
    "ref_word = []\n",
    "\n",
    "print(\"Translating and evaluating sentences...\")\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    german_sentence = row['generated_context']\n",
    "    loan_word = row['original_word']\n",
    "    \n",
    "    english_translation = generate_reference_sentence(german_sentence)\n",
    "    translated_sentence.append(english_translation)\n",
    "\n",
    "    reference_sentence = translator(german_sentence)[0]['translation_text']\n",
    "    ref.append(reference_sentence)\n",
    "\n",
    "    reference_word = translator(loan_word)[0]['translation_text']\n",
    "    ref_word.append(reference_word)\n",
    "    \n",
    "\n",
    "    bleu_score = calculate_bleu(reference_sentence, english_translation)\n",
    "    bleu_scores.append(bleu_score)\n",
    "    \n",
    "    meteor_score_value = calculate_meteor(reference_sentence, english_translation)\n",
    "    meteor_scores.append(meteor_score_value)\n",
    "    \n",
    "    # print(f\"Loanword: {loan_word}\")\n",
    "    # print(f\"German Sentence: {german_sentence}\")\n",
    "    # print(f\"Translated Sentence: {english_translation}\")\n",
    "    # print(f\"Reference Sentence: {reference_sentence}\")\n",
    "    # print(f\"BLEU Score: {bleu_score:.4f}, METEOR Score: {meteor_score_value:.4f}\")\n",
    "\n",
    "\n",
    "df['reference_sentence'] = ref\n",
    "df[\"translated_sentence\"] = translated_sentence\n",
    "df['bleu_score'] = bleu_scores\n",
    "df['meteor_score'] = meteor_scores\n",
    "df['reference_word'] = ref_word\n",
    "\n",
    "important_columns = ['loan_word', 'original_word', 'generated_context', 'reference_sentence', 'translated_sentence',\"reference_word\",'label' ,'bleu_score', 'meteor_score']\n",
    "df_important = df[important_columns]\n",
    "\n",
    "output_path = 'loanwords_with_context_and_scores.csv'\n",
    "df_important.to_csv(output_path, index=False)\n",
    "print(f\"Saved with context and scores to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "792aac9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_word</th>\n",
       "      <th>original_word</th>\n",
       "      <th>generated_context</th>\n",
       "      <th>reference_sentence</th>\n",
       "      <th>translated_sentence</th>\n",
       "      <th>reference_word</th>\n",
       "      <th>label</th>\n",
       "      <th>bleu_score</th>\n",
       "      <th>meteor_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mirth</td>\n",
       "      <td>Fröhlichkeit</td>\n",
       "      <td>Die Weihnachtsfeier strahlte vor Fröhlichkeit.</td>\n",
       "      <td>The Christmas party shone with joy.</td>\n",
       "      <td>The Christmas party radiated joyfulness.</td>\n",
       "      <td>Cheerfulness</td>\n",
       "      <td>synonym</td>\n",
       "      <td>0.274825</td>\n",
       "      <td>0.499058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Schnorr</td>\n",
       "      <td>Chromatogramm</td>\n",
       "      <td>Das Chromatogramm zeigt die verschiedenen Best...</td>\n",
       "      <td>The chromatogram shows the various components ...</td>\n",
       "      <td>The chromatogram shows the various components ...</td>\n",
       "      <td>Chromatogram</td>\n",
       "      <td>random</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Zettelkasten</td>\n",
       "      <td>Zettelkasten</td>\n",
       "      <td>Ich habe endlich meinen Zettelkasten aufgeräumt.</td>\n",
       "      <td>I finally cleaned up my paperbox.</td>\n",
       "      <td>I have finally tidied up my slip box.</td>\n",
       "      <td>Paper box</td>\n",
       "      <td>loan</td>\n",
       "      <td>0.119901</td>\n",
       "      <td>0.509073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Meiring</td>\n",
       "      <td>Meiring</td>\n",
       "      <td>Meiring ist ein bekannter deutscher Familienname.</td>\n",
       "      <td>Meiring is a well-known German surname.</td>\n",
       "      <td>Meiring is a well-known German surname.</td>\n",
       "      <td>Meiring</td>\n",
       "      <td>loan</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Speth</td>\n",
       "      <td>Speth</td>\n",
       "      <td>Herr Speth ist unser neuer Nachbar.</td>\n",
       "      <td>Mr. Speth is our new neighbor.</td>\n",
       "      <td>Mr. Speth is our new neighbor.</td>\n",
       "      <td>Speth</td>\n",
       "      <td>loan</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Important</td>\n",
       "      <td>Gefeiert</td>\n",
       "      <td>Das Jubiläum wurde gestern Abend groß gefeiert.</td>\n",
       "      <td>The anniversary was celebrated very much last ...</td>\n",
       "      <td>The anniversary was celebrated in a big way la...</td>\n",
       "      <td>Celebrated</td>\n",
       "      <td>synonym</td>\n",
       "      <td>0.339325</td>\n",
       "      <td>0.718157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Breisgau</td>\n",
       "      <td>Ennen</td>\n",
       "      <td>Ich war schon ennen hier, als das Restaurant n...</td>\n",
       "      <td>I was already here when the restaurant didn't ...</td>\n",
       "      <td>I was already here before the restaurant existed.</td>\n",
       "      <td>Eureka</td>\n",
       "      <td>random</td>\n",
       "      <td>0.295387</td>\n",
       "      <td>0.600907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Abstruse</td>\n",
       "      <td>Unklar</td>\n",
       "      <td>Die Bedingungen des Vertrags sind mir unklar.</td>\n",
       "      <td>The terms of the contract are unclear to me.</td>\n",
       "      <td>The conditions of the contract are unclear to me.</td>\n",
       "      <td>Unclear</td>\n",
       "      <td>synonym</td>\n",
       "      <td>0.782542</td>\n",
       "      <td>0.881944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Uncommon</td>\n",
       "      <td>Fantastisch</td>\n",
       "      <td>Das Konzert war fantastisch, ich habe mich seh...</td>\n",
       "      <td>The concert was fantastic, I had a very good c...</td>\n",
       "      <td>The concert was fantastic, I had a really grea...</td>\n",
       "      <td>Fantastic</td>\n",
       "      <td>synonym</td>\n",
       "      <td>0.631555</td>\n",
       "      <td>0.698980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Wemhoff</td>\n",
       "      <td>Gut</td>\n",
       "      <td>Das ist gut.</td>\n",
       "      <td>That's good.</td>\n",
       "      <td>That is good.</td>\n",
       "      <td>Good</td>\n",
       "      <td>random</td>\n",
       "      <td>0.319472</td>\n",
       "      <td>0.238095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      loan_word  original_word  \\\n",
       "0         Mirth   Fröhlichkeit   \n",
       "1       Schnorr  Chromatogramm   \n",
       "2  Zettelkasten   Zettelkasten   \n",
       "3       Meiring        Meiring   \n",
       "4         Speth          Speth   \n",
       "5     Important       Gefeiert   \n",
       "6      Breisgau          Ennen   \n",
       "7      Abstruse         Unklar   \n",
       "8      Uncommon    Fantastisch   \n",
       "9       Wemhoff            Gut   \n",
       "\n",
       "                                   generated_context  \\\n",
       "0     Die Weihnachtsfeier strahlte vor Fröhlichkeit.   \n",
       "1  Das Chromatogramm zeigt die verschiedenen Best...   \n",
       "2   Ich habe endlich meinen Zettelkasten aufgeräumt.   \n",
       "3  Meiring ist ein bekannter deutscher Familienname.   \n",
       "4                Herr Speth ist unser neuer Nachbar.   \n",
       "5    Das Jubiläum wurde gestern Abend groß gefeiert.   \n",
       "6  Ich war schon ennen hier, als das Restaurant n...   \n",
       "7      Die Bedingungen des Vertrags sind mir unklar.   \n",
       "8  Das Konzert war fantastisch, ich habe mich seh...   \n",
       "9                                       Das ist gut.   \n",
       "\n",
       "                                  reference_sentence  \\\n",
       "0                The Christmas party shone with joy.   \n",
       "1  The chromatogram shows the various components ...   \n",
       "2                  I finally cleaned up my paperbox.   \n",
       "3            Meiring is a well-known German surname.   \n",
       "4                     Mr. Speth is our new neighbor.   \n",
       "5  The anniversary was celebrated very much last ...   \n",
       "6  I was already here when the restaurant didn't ...   \n",
       "7       The terms of the contract are unclear to me.   \n",
       "8  The concert was fantastic, I had a very good c...   \n",
       "9                                       That's good.   \n",
       "\n",
       "                                 translated_sentence reference_word    label  \\\n",
       "0           The Christmas party radiated joyfulness.   Cheerfulness  synonym   \n",
       "1  The chromatogram shows the various components ...   Chromatogram   random   \n",
       "2              I have finally tidied up my slip box.      Paper box     loan   \n",
       "3            Meiring is a well-known German surname.        Meiring     loan   \n",
       "4                     Mr. Speth is our new neighbor.          Speth     loan   \n",
       "5  The anniversary was celebrated in a big way la...     Celebrated  synonym   \n",
       "6  I was already here before the restaurant existed.         Eureka   random   \n",
       "7  The conditions of the contract are unclear to me.        Unclear  synonym   \n",
       "8  The concert was fantastic, I had a really grea...      Fantastic  synonym   \n",
       "9                                      That is good.           Good   random   \n",
       "\n",
       "   bleu_score  meteor_score  \n",
       "0    0.274825      0.499058  \n",
       "1    1.000000      0.999314  \n",
       "2    0.119901      0.509073  \n",
       "3    1.000000      0.997685  \n",
       "4    1.000000      0.997685  \n",
       "5    0.339325      0.718157  \n",
       "6    0.295387      0.600907  \n",
       "7    0.782542      0.881944  \n",
       "8    0.631555      0.698980  \n",
       "9    0.319472      0.238095  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7b9649eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14707/593377672.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model/loan_word_model.pth\", map_location=torch.device('cpu')))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "class LoanWordClassifier(torch.nn.Module):\n",
    "    def __init__(self, num_phonetic_embeddings, num_labels=2):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"model/tuned-bert\")\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.phonetic_embedder = torch.nn.Embedding(num_phonetic_embeddings, embedding_dim=64)\n",
    "        \n",
    "        bert_hidden_size = self.bert.config.hidden_size \n",
    "        phonetic_size = 64\n",
    "        unicode_size = 25 \n",
    "        other_size = 1\n",
    "        \n",
    "        total_input_size = bert_hidden_size + phonetic_size + unicode_size + other_size\n",
    "        \n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(total_input_size, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, phonetic_seq, unicode_feature, other_feature):\n",
    "        \n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_states = outputs.last_hidden_state \n",
    "        pooled_output = last_hidden_states.mean(dim=1) \n",
    "        phonetic_emb = self.phonetic_embedder(phonetic_seq).mean(dim=1) \n",
    "\n",
    "    \n",
    "        unicode_feature = unicode_feature.view(unicode_feature.size(0), -1)  \n",
    "        other_feature = other_feature.view(other_feature.size(0), -1)      \n",
    "\n",
    "    \n",
    "        combined = torch.cat([\n",
    "            pooled_output,       \n",
    "            phonetic_emb,        \n",
    "            unicode_feature,     \n",
    "            other_feature        \n",
    "        ], dim=1)               \n",
    "        \n",
    "        logits = self.classifier(combined)\n",
    "        return logits\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "import epitran\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = LoanWordClassifier(111024)\n",
    "model.load_state_dict(torch.load(\"model/loan_word_model.pth\", map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "tokenizer = BertTokenizer.from_pretrained(\"model/tuned-bert-tokenizer\")\n",
    "epi = epitran.Epitran(\"fra-Latn\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def normalize(unicode_values):\n",
    "    mean_value = sum(unicode_values) / len(unicode_values)\n",
    "    return [val - mean_value for val in unicode_values]\n",
    "\n",
    "def extract_features(word, max_len=25):\n",
    "    try:\n",
    "        loan_epitran = epi.transliterate(word)\n",
    "        phonetic_seq = [ord(c) for c in loan_epitran] \n",
    "    except IndexError as e:\n",
    "        print(f\"Transliteration failed for '{word}': {e}\")\n",
    "        phonetic_seq = [0] \n",
    "\n",
    "    unicode_features = [ord(c) for c in word]\n",
    "    unicode_features = normalize(unicode_features)\n",
    "\n",
    "    if len(unicode_features) < max_len:\n",
    "        unicode_features = unicode_features + [0] * (max_len - len(unicode_features))\n",
    "    else:\n",
    "        unicode_features = unicode_features[:max_len]  # Truncate if longer\n",
    "\n",
    "    return phonetic_seq, unicode_features, [len(word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "afdbab19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensor([[0.7009, 0.2991]])\n",
      "Christmas tensor([[0.7576, 0.2424]])\n",
      "party tensor([[0.5908, 0.4092]])\n",
      "radiated tensor([[0.5134, 0.4866]])\n",
      "joyfulness tensor([[0.6997, 0.3003]])\n",
      "False loan words: []\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "import epitran\n",
    "\n",
    "# Load model and tokenizer\n",
    "model.eval()\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "epi = epitran.Epitran(\"deu-Latn\")\n",
    "\n",
    "def normalize(unicode_values):\n",
    "    mean_value = sum(unicode_values) / len(unicode_values)\n",
    "    return [val - mean_value for val in unicode_values]\n",
    "\n",
    "def extract_features(word, max_len=25):\n",
    "    try:\n",
    "        loan_epitran = epi.transliterate(word)\n",
    "        phonetic_seq = [ord(c) for c in loan_epitran] \n",
    "    except IndexError as e:\n",
    "        print(f\"Transliteration failed for '{word}': {e}\")\n",
    "        phonetic_seq = [0] \n",
    "\n",
    "    unicode_features = [ord(c) for c in word]\n",
    "    unicode_features = normalize(unicode_features)\n",
    "\n",
    "    if len(unicode_features) < max_len:\n",
    "        unicode_features = unicode_features + [0] * (max_len - len(unicode_features))\n",
    "    else:\n",
    "        unicode_features = unicode_features[:max_len]  # Truncate if longer\n",
    "\n",
    "    return phonetic_seq, unicode_features, [len(word)]\n",
    "\n",
    "\n",
    "\n",
    "sentence = \"The Christmas party radiated joyfulness\"\n",
    "words = sentence.split()\n",
    "\n",
    "false_loans = []\n",
    "for word in words:\n",
    "    phonetic_seq, unicode_feature, other_feature = extract_features(word)\n",
    "    inputs = tokenizer(word, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(\n",
    "            input_ids=inputs[\"input_ids\"].to(device),\n",
    "            attention_mask=inputs[\"attention_mask\"].to(device),\n",
    "            phonetic_seq=torch.tensor([phonetic_seq], dtype=torch.long).to(device),\n",
    "            unicode_feature=torch.tensor([unicode_feature], dtype=torch.float).to(device),\n",
    "            other_feature=torch.tensor([other_feature], dtype=torch.float).to(device)\n",
    "        )\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        print(word , probs)\n",
    "        \n",
    "        if torch.argmax(probs) == 1:  \n",
    "            false_loans.append(word)\n",
    "\n",
    "print(\"False loan words:\", false_loans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87359073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ea4964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e30cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba23d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe307e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
